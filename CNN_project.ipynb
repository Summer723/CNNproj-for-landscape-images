{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dadb02df-4542-4286-913c-d0dc6c4c1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn.functional import one_hot as one_hot_encoding\n",
    "import matplotlib.pyplot as pltd\n",
    "import import_ipynb\n",
    "from CNN_model import Net\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9eb31f1c-533a-4fa2-85da-a1368122f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/summer/desktop/machine learning project/data/UCMerced_LandUse/Images\"\n",
    "category = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0cdf03d-7578-46ea-99b0-ee8e92d8e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_names = [\n",
    "    'agricultural',\n",
    "    'forest',\n",
    "    'beach',\n",
    "    'runway',\n",
    "    'river',\n",
    "]\n",
    "num_images = 100\n",
    "nb_classes = len(category_names)\n",
    "patch_size = 256\n",
    "channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d0a85e2-4f3a-48c8-a3ea-245dd9119033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # some experiments \n",
    "# new_image = Image.open(\"/Users/summer/desktop/machine learning project/data/UCMerced_LandUse/Images/agricultural/agricultural00.tif\")\n",
    "# image = ToTensor()(new_image)\n",
    "# print(image.shape)\n",
    "# new_image = fn.resize(image, size=[patch_size//4, patch_size//4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b77d1d56-9be1-4e52-b8e5-291004afec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we take out the data and transform them \n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(category_names)):\n",
    "    for j in range(num_images):\n",
    "        image = Image.open(\"/Users/summer/desktop/machine learning project/data/UCMerced_LandUse/Images/{}/{}{:02d}.tif\".format(category_names[i],category_names[i],j))\n",
    "        \n",
    "        # make it a tensor\n",
    "        image = ToTensor()(image)\n",
    "        # resize it \n",
    "        image = fn.resize(image, size=[patch_size//4, patch_size//4])\n",
    "\n",
    "        image_arr = np.asarray(image)\n",
    "\n",
    "        X.append(image_arr)\n",
    "        y.append([i])\n",
    "\n",
    "        \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)\n",
    "\n",
    "# we don't want labels to be 1, 2 ,3 ,\n",
    "# we want it in the format of one hot encoding\n",
    "y = one_hot_encoding(y)\n",
    "\n",
    "torch.save(X, \"/Users/summer/desktop/machine learning project/datasets.pt\")\n",
    "torch.save(y, \"/Users/summer/desktop/machine learning project/labels.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a988e737-4dc9-41be-8063-4651274cc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have to split the dataset into training, validation and testing \n",
    "dataset_length = X.shape[0]\n",
    "shuffled_indices = np.arange(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5d861b7-28fb-45ef-9680-488102b9757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "np.random.shuffle(shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e366a444-ded6-4542-bb3e-02404e9949b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_indices, validation_indices, test_indices = shuffled_indices[0:int(dataset_length*0.6)],\\\n",
    "                                                     shuffled_indices[int(dataset_length*0.6):int(dataset_length * 0.8)],\\\n",
    "                                                     shuffled_indices[int(dataset_length * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "801c1a59-efdb-45b8-9a14-00eb5b5a5771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "training_set, validation_set, test_set = X[training_indices],X[validation_indices],X[test_indices]\n",
    "training_label, validation_label, test_label = y[training_indices], y[validation_indices], y[test_indices]\n",
    "print(training_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be971415-1afd-4f98-8f71-d2d3f195f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, (3, 3),padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3,3),padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.fc = nn.Linear(16384, 5)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x) # flatten all dimensions except batch\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "22bb278b-e25e-4105-8d06-7673b422c6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 4, 3, 0, 1, 4, 1, 1, 1, 1, 0, 1, 4, 1, 4, 3, 2, 1, 4, 1, 4, 0, 2,\n",
      "        2, 2, 1, 3, 2, 0, 1, 3, 4, 3, 0, 3, 4, 1, 0, 0, 1, 4, 2, 2, 1, 2, 4, 4,\n",
      "        4, 3, 4, 3, 3, 4, 0, 2, 4, 0, 4, 4, 2, 3, 1, 1, 3, 1, 1, 3, 2, 1, 0, 3,\n",
      "        3, 0, 0, 3, 2, 2, 0, 4, 0, 1, 4, 3, 2, 0, 0, 0, 1, 4, 1, 3, 3, 4, 1, 4,\n",
      "        1, 1, 0, 3, 2, 4, 3, 4, 0, 1, 1, 0, 4, 0, 3, 4, 4, 4, 4, 3, 2, 2, 3, 2,\n",
      "        0, 0, 0, 3, 4, 2, 3, 3, 0, 2, 0, 1, 2, 4, 0, 0, 2, 1, 3, 4, 0, 3, 2, 1,\n",
      "        4, 3, 1, 4, 0, 3, 1, 0, 3, 4, 1, 4, 2, 4, 2, 0, 0, 2, 2, 4, 1, 2, 4, 3,\n",
      "        4, 0, 1, 2, 2, 0, 4, 1, 4, 0, 1, 1, 3, 1, 2, 4, 4, 1, 1, 0, 3, 4, 4, 4,\n",
      "        4, 3, 3, 1, 2, 0, 2, 0, 4, 2, 3, 1, 2, 0, 1, 0, 3, 3, 4, 0, 1, 0, 3, 3,\n",
      "        1, 4, 2, 4, 3, 4, 4, 2, 2, 1, 2, 0, 0, 0, 4, 0, 2, 3, 3, 4, 2, 0, 3, 2,\n",
      "        2, 3, 4, 0, 4, 3, 4, 2, 1, 2, 0, 3, 2, 1, 1, 1, 3, 3, 3, 3, 2, 2, 0, 2,\n",
      "        1, 4, 0, 3, 4, 1, 3, 0, 2, 1, 0, 1, 4, 1, 1, 3, 4, 3, 2, 1, 2, 1, 2, 2,\n",
      "        0, 0, 3, 4, 4, 4, 4, 3, 4, 2, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "output = model.forward(training_set)\n",
    "int_label = torch.argmax(training_label, dim=2).reshape(300,)\n",
    "print(int_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d3318d7c-36c5-40fc-b41e-ac4c6e3585b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6089, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "print(loss(output, int_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bb6604ec-a71c-4105-b6bc-ea2a6cf00a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4083, -0.2705,  1.4404,  0.2484,  0.5575],\n",
      "        [-0.3670, -0.9370,  2.2625, -0.3673, -2.1630],\n",
      "        [ 0.2162,  0.8056,  0.4344, -1.2499,  1.0685]], requires_grad=True)\n",
      "tensor([2, 3, 0])\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba7811a-6e5e-447e-a6aa-8518df8feac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389ddc8-a58e-4876-97a7-91862ba2b272",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
