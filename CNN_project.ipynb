{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dadb02df-4542-4286-913c-d0dc6c4c1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn.functional import one_hot as one_hot_encoding\n",
    "import matplotlib.pyplot as pltd\n",
    "import import_ipynb\n",
    "\n",
    "from torchsummary import summary\n",
    "import landscapedataset\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "657cc984-76bb-4ebd-b674-620648de1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = 256\n",
    "category = {\n",
    "            'forest': 0, 'buildings': 1, 'river': 2, 'mobilehomepark': 3,\n",
    "             'harbor': 4, 'golfcourse': 5, 'agricultural': 6, \n",
    "             'runway': 7, 'baseballdiamond' : 8, 'overpass' : 9, 'chaparral':10, \n",
    "             'tenniscourt':11, 'intersection':12, 'airplane':13, 'parkinglot':14, \n",
    "             'sparseresidential':15, 'mediumresidential':16, 'denseresidential':17, \n",
    "             'beach':18, 'freeway':19, 'storagetanks':20\n",
    "}\n",
    "path = \"/Users/summer/desktop/CNNproj-for-landscape-images/data/UCMerced_LandUse/WholeDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d554170b-22c2-46c9-a8d1-5f85d8b99049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset I created \n",
    "# easier to use dataloader \n",
    "\n",
    "class LandscapeDataset(Dataset):\n",
    "    # here label is a 100 element tensor\n",
    "    def __init__(self, label, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = label\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_data = self.img_dir + \"/\" + os.listdir(self.img_dir)[idx]\n",
    "        image = Image.open(path_to_data)\n",
    "        \n",
    "        # make it a tensor\n",
    "        image = ToTensor()(image)\n",
    "        label = self.img_labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image,size=[patch//4, patch//4])\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9eb31f1c-533a-4fa2-85da-a1368122f0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {\n",
    "            'forest': 0, 'buildings': 1, 'river': 2, 'mobilehomepark': 3,\n",
    "             'harbor': 4, 'golfcourse': 5, 'agricultural': 6, \n",
    "             'runway': 7, 'baseballdiamond' : 8, 'overpass' : 9, 'chaparral':10, \n",
    "             'tenniscourt':11, 'intersection':12, 'airplane':13, 'parkinglot':14, \n",
    "             'sparseresidential':15, 'mediumresidential':16, 'denseresidential':17, \n",
    "             'beach':18, 'freeway':19, 'storagetanks':20\n",
    "}\n",
    "path = \"/Users/summer/desktop/CNNproj-for-landscape-images/data/UCMerced_LandUse/WholeDataset\"\n",
    "\n",
    "data = os.listdir(path)\n",
    "labels = torch.zeros(len(data))\n",
    "# try to find the class that image belongs to\n",
    "for i in range (len(data)):\n",
    "    labels[i] = category[re.findall('([a-zA-Z ]+)\\d*.*', data[i])[0]]\n",
    "dataset = LandscapeDataset(\n",
    "                            labels, \n",
    "                           \"/Users/summer/desktop/CNNproj-for-landscape-images/data/UCMerced_LandUse/WholeDataset\",\n",
    "                            transform = fn.resize\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b3904bac-3ad5-46c7-a23b-e281df7153e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set, test_set = torch.utils.data.random_split(dataset, \n",
    "                                                                       [1260, 420, 420], \n",
    "                                                                       generator=torch.Generator().manual_seed(42)\n",
    "                                                                      )\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=32, shuffle=True,drop_last =True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=32, shuffle=False,drop_last =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66d08fca-dcaa-4637-98e7-750d6355c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        pass\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0cb269f1-30bb-4f3d-8d3d-f838a1d87221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, (3, 3),padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3,3),padding='same')\n",
    "        self.conv3 = nn.Conv2d(64,128,(3,3), padding=\"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.fc = nn.Linear(8192, 21)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.flatten(x) # flatten all dimensions except batch\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0850c184-b2f8-4262-a523-86a15d470ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 64, 64]             896\n",
      "              ReLU-2           [-1, 32, 64, 64]               0\n",
      "         MaxPool2d-3           [-1, 32, 32, 32]               0\n",
      "            Conv2d-4           [-1, 64, 32, 32]          18,496\n",
      "              ReLU-5           [-1, 64, 32, 32]               0\n",
      "         MaxPool2d-6           [-1, 64, 16, 16]               0\n",
      "            Conv2d-7          [-1, 128, 16, 16]          73,856\n",
      "              ReLU-8          [-1, 128, 16, 16]               0\n",
      "         MaxPool2d-9            [-1, 128, 8, 8]               0\n",
      "          Flatten-10                 [-1, 8192]               0\n",
      "           Linear-11                   [-1, 21]         172,053\n",
      "          Dropout-12                   [-1, 21]               0\n",
      "          Softmax-13                   [-1, 21]               0\n",
      "================================================================\n",
      "Total params: 265,301\n",
      "Trainable params: 265,301\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 4.00\n",
      "Params size (MB): 1.01\n",
      "Estimated Total Size (MB): 5.06\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(Net(), input_size=( 3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b509cb24-cb86-43ac-8599-39a53fd2d6a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac3acb73-18e7-4d55-905c-bc708cc2a7d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46d83ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(training_loader) + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fb8f7-2e8c-4bbb-9305-c2df786e9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be26bb0e-d422-407c-ae51-e86d95239300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0433df3c",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3720.4043, grad_fn=<DivBackward1>)\n",
      "tensor(1894.0665, grad_fn=<DivBackward1>)\n",
      "tensor(1989.0452, grad_fn=<DivBackward1>)\n",
      "tensor(3982.6130, grad_fn=<DivBackward1>)\n",
      "tensor(3937.0732, grad_fn=<DivBackward1>)\n",
      "tensor(2648.7676, grad_fn=<DivBackward1>)\n",
      "tensor(4000.3257, grad_fn=<DivBackward1>)\n",
      "tensor(3687.7312, grad_fn=<DivBackward1>)\n",
      "tensor(2623.5073, grad_fn=<DivBackward1>)\n",
      "tensor(1959.9724, grad_fn=<DivBackward1>)\n",
      "  batch 10 loss: 3044.3506469726562\n",
      "tensor(1656.9861, grad_fn=<DivBackward1>)\n",
      "tensor(2367.3284, grad_fn=<DivBackward1>)\n",
      "tensor(3456.4707, grad_fn=<DivBackward1>)\n",
      "tensor(3652.4890, grad_fn=<DivBackward1>)\n",
      "tensor(4831.3623, grad_fn=<DivBackward1>)\n",
      "tensor(3008.7566, grad_fn=<DivBackward1>)\n",
      "tensor(2198.7393, grad_fn=<DivBackward1>)\n",
      "tensor(1785.4066, grad_fn=<DivBackward1>)\n",
      "tensor(3381.3853, grad_fn=<DivBackward1>)\n",
      "tensor(3651.7817, grad_fn=<DivBackward1>)\n",
      "  batch 20 loss: 2999.070593261719\n",
      "tensor(3563.2051, grad_fn=<DivBackward1>)\n",
      "tensor(2272.5081, grad_fn=<DivBackward1>)\n",
      "tensor(4145.3120, grad_fn=<DivBackward1>)\n",
      "tensor(2510.4197, grad_fn=<DivBackward1>)\n",
      "tensor(4203.1963, grad_fn=<DivBackward1>)\n",
      "tensor(4146.9155, grad_fn=<DivBackward1>)\n",
      "tensor(3439.1035, grad_fn=<DivBackward1>)\n",
      "tensor(2888.2671, grad_fn=<DivBackward1>)\n",
      "tensor(3318.4470, grad_fn=<DivBackward1>)\n",
      "tensor(1996.9677, grad_fn=<DivBackward1>)\n",
      "  batch 30 loss: 3248.434191894531\n",
      "tensor(2413.0259, grad_fn=<DivBackward1>)\n",
      "tensor(3895.8926, grad_fn=<DivBackward1>)\n",
      "tensor(3704.8650, grad_fn=<DivBackward1>)\n",
      "tensor(2683.6323, grad_fn=<DivBackward1>)\n",
      "tensor(3110.1328, grad_fn=<DivBackward1>)\n",
      "tensor(2383.8242, grad_fn=<DivBackward1>)\n",
      "tensor(4117.6855, grad_fn=<DivBackward1>)\n",
      "tensor(1858.5420, grad_fn=<DivBackward1>)\n",
      "tensor(3008.2634, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0.\n",
    "last_loss = 0.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# Here, we use enumerate(training_loader) instead of\n",
    "# iter(training_loader) so that we can track the batch\n",
    "# index and do some intra-epoch reporting\n",
    "for i, data in enumerate(training_loader):\n",
    "    # Every data instance is an input + label pair\n",
    "    inputs, labels = data\n",
    "\n",
    "    # Zero your gradients for every batch!\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Make predictions for this batch\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Compute the loss and its gradients\n",
    "    outputs = torch.argmax(outputs, dim=1)\n",
    "    outputs = outputs.to(torch.float)\n",
    "    outputs.requires_grad = True\n",
    "    \n",
    "    loss = loss_fn(outputs, labels)\n",
    "    print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    # Adjust learning weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Gather data and report\n",
    "    running_loss += loss.item()\n",
    "    if i % 10  == 9:\n",
    "        last_loss = running_loss / 10 # loss per batch\n",
    "        print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "        running_loss = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e0cdf03d-7578-46ea-99b0-ee8e92d8e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_names = [\n",
    "#     'agricultural',\n",
    "#     'forest',\n",
    "#     'beach',\n",
    "#     'runway',\n",
    "#     'river',\n",
    "# ]\n",
    "# num_images = 100\n",
    "# nb_classes = len(category_names)\n",
    "# patch_size = 256\n",
    "# channels = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a85e2-4f3a-48c8-a3ea-245dd9119033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # some experiments \n",
    "# new_image = Image.open(\"/Users/summer/desktop/machine learning project/data/UCMerced_LandUse/Images/agricultural/agricultural00.tif\")\n",
    "# image = ToTensor()(new_image)\n",
    "# print(image.shape)\n",
    "# new_image = fn.resize(image, size=[patch_size//4, patch_size//4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d1d56-9be1-4e52-b8e5-291004afec9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first we take out the data and transform them \n",
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# for i in range(len(category_names)):\n",
    "#     for j in range(num_images):\n",
    "#         image = Image.open(\"/Users/summer/desktop/machine learning project/data/UCMerced_LandUse/Images/{}/{}{:02d}.tif\".format(category_names[i],category_names[i],j))\n",
    "        \n",
    "#         # make it a tensor\n",
    "#         image = ToTensor()(image)\n",
    "#         # resize it \n",
    "#         image = fn.resize(image, size=[patch_size//4, patch_size//4])\n",
    "\n",
    "#         image_arr = np.asarray(image)\n",
    "\n",
    "#         X.append(image_arr)\n",
    "#         y.append([i])\n",
    "\n",
    "        \n",
    "\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "# X = torch.from_numpy(X)\n",
    "# y = torch.from_numpy(y)\n",
    "\n",
    "# # we don't want labels to be 1, 2 ,3 ,\n",
    "# # we want it in the format of one hot encoding\n",
    "# y = one_hot_encoding(y)\n",
    "\n",
    "# torch.save(X, \"/Users/summer/desktop/machine learning project/datasets.pt\")\n",
    "# torch.save(y, \"/Users/summer/desktop/machine learning project/labels.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988e737-4dc9-41be-8063-4651274cc822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now we have to split the dataset into training, validation and testing \n",
    "# dataset_length = X.shape[0]\n",
    "# shuffled_indices = np.arange(dataset_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d861b7-28fb-45ef-9680-488102b9757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(100)\n",
    "# np.random.shuffle(shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e366a444-ded6-4542-bb3e-02404e9949b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_indices, validation_indices, test_indices = shuffled_indices[0:int(dataset_length*0.6)],\\\n",
    "#                                                      shuffled_indices[int(dataset_length*0.6):int(dataset_length * 0.8)],\\\n",
    "#                                                      shuffled_indices[int(dataset_length * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c1a59-efdb-45b8-9a14-00eb5b5a5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set, validation_set, test_set = X[training_indices],X[validation_indices],X[test_indices]\n",
    "# training_label, validation_label, test_label = y[training_indices], y[validation_indices], y[test_indices]\n",
    "# print(training_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "be971415-1afd-4f98-8f71-d2d3f195f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, (3, 3),padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3,3),padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.fc = nn.Linear(16384, 21)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x) # flatten all dimensions except batch\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb278b-e25e-4105-8d06-7673b422c6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3318d7c-36c5-40fc-b41e-ac4c6e3585b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "print(loss(output, int_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6604ec-a71c-4105-b6bc-ea2a6cf00a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dba7811a-6e5e-447e-a6aa-8518df8feac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1539, -0.5191, -2.0220,  1.0171, -1.1370],\n",
      "        [ 0.1893,  2.3678,  0.0744,  1.1048,  0.0262],\n",
      "        [ 0.3962, -0.3478, -1.3087, -0.7338,  0.1521]], requires_grad=True)\n",
      "tensor([0, 4, 0])\n",
      "tensor(2.7758, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "print(input)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "output.backward()\n",
    "# Example of target with class probabilities\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5).softmax(dim=1)\n",
    "output = loss(input, target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d389ddc8-a58e-4876-97a7-91862ba2b272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8314, 0.8294, 0.8167,  ..., 0.6931, 0.6637, 0.6304],\n",
       "          [0.8314, 0.8216, 0.8098,  ..., 0.6255, 0.6441, 0.7108],\n",
       "          [0.8186, 0.8108, 0.8137,  ..., 0.6647, 0.7422, 0.7304],\n",
       "          ...,\n",
       "          [0.7990, 0.8118, 0.7931,  ..., 0.6196, 0.6255, 0.6137],\n",
       "          [0.7892, 0.7912, 0.7431,  ..., 0.6147, 0.6206, 0.6363],\n",
       "          [0.7902, 0.8167, 0.7637,  ..., 0.6088, 0.6098, 0.6373]],\n",
       " \n",
       "         [[0.8314, 0.8304, 0.8176,  ..., 0.7069, 0.6765, 0.6412],\n",
       "          [0.8314, 0.8235, 0.8137,  ..., 0.6422, 0.6618, 0.7324],\n",
       "          [0.8167, 0.8147, 0.8157,  ..., 0.6873, 0.7510, 0.7461],\n",
       "          ...,\n",
       "          [0.8088, 0.8127, 0.7961,  ..., 0.6422, 0.6382, 0.6324],\n",
       "          [0.7931, 0.7990, 0.7471,  ..., 0.6333, 0.6333, 0.6461],\n",
       "          [0.7971, 0.8206, 0.7588,  ..., 0.6265, 0.6314, 0.6588]],\n",
       " \n",
       "         [[0.7922, 0.7863, 0.7784,  ..., 0.6343, 0.6059, 0.5784],\n",
       "          [0.7843, 0.7755, 0.7667,  ..., 0.5696, 0.6059, 0.6765],\n",
       "          [0.7706, 0.7696, 0.7686,  ..., 0.6294, 0.6863, 0.6873],\n",
       "          ...,\n",
       "          [0.7873, 0.7912, 0.7667,  ..., 0.5402, 0.5392, 0.5314],\n",
       "          [0.7755, 0.7863, 0.7157,  ..., 0.5441, 0.5373, 0.5510],\n",
       "          [0.7784, 0.8029, 0.7373,  ..., 0.5206, 0.5353, 0.5657]]]),\n",
       " tensor(18.))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "fdbe8479-4a60-4cde-b4f7-c41dbc64b297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3320, 0.3468, 0.3189, 0.3240, 0.3231, 0.3258, 0.3291, 0.3215, 0.3272,\n",
      "         0.3217, 0.3239, 0.3341, 0.3352, 0.3416, 0.3394, 0.3317, 0.3315, 0.3268,\n",
      "         0.3275, 0.3264, 0.3311, 0.3228, 0.3240, 0.3242, 0.3248, 0.3210, 0.3220,\n",
      "         0.3184, 0.3252, 0.3264, 0.3258, 0.3222, 0.3196, 0.3221, 0.3290, 0.3230,\n",
      "         0.3260, 0.3242, 0.3202, 0.3189, 0.3180, 0.3178, 0.3207, 0.3203, 0.3204,\n",
      "         0.3153, 0.3204, 0.3167, 0.3185, 0.3244, 0.3265, 0.3344, 0.3372, 0.3462,\n",
      "         0.3323, 0.3307, 0.3246, 0.3138, 0.3243, 0.3276, 0.3248, 0.3324, 0.3307,\n",
      "         0.3349],\n",
      "        [0.3437, 0.3597, 0.3354, 0.3425, 0.3424, 0.3435, 0.3471, 0.3394, 0.3466,\n",
      "         0.3449, 0.3462, 0.3564, 0.3566, 0.3593, 0.3569, 0.3488, 0.3511, 0.3497,\n",
      "         0.3489, 0.3459, 0.3500, 0.3432, 0.3449, 0.3462, 0.3472, 0.3409, 0.3414,\n",
      "         0.3386, 0.3461, 0.3478, 0.3492, 0.3463, 0.3436, 0.3468, 0.3531, 0.3451,\n",
      "         0.3455, 0.3482, 0.3456, 0.3442, 0.3432, 0.3459, 0.3490, 0.3475, 0.3472,\n",
      "         0.3412, 0.3452, 0.3402, 0.3415, 0.3462, 0.3473, 0.3536, 0.3558, 0.3630,\n",
      "         0.3504, 0.3500, 0.3441, 0.3337, 0.3435, 0.3464, 0.3431, 0.3496, 0.3489,\n",
      "         0.3543],\n",
      "        [0.3007, 0.3054, 0.2944, 0.3083, 0.3069, 0.3064, 0.3097, 0.3033, 0.3117,\n",
      "         0.3095, 0.3087, 0.3182, 0.3180, 0.3218, 0.3218, 0.3168, 0.3189, 0.3167,\n",
      "         0.3168, 0.3144, 0.3152, 0.3072, 0.3097, 0.3104, 0.3151, 0.3077, 0.3067,\n",
      "         0.3049, 0.3136, 0.3167, 0.3165, 0.3116, 0.3095, 0.3116, 0.3170, 0.3092,\n",
      "         0.3115, 0.3119, 0.3087, 0.3078, 0.3081, 0.3110, 0.3139, 0.3126, 0.3118,\n",
      "         0.3025, 0.3087, 0.3048, 0.3045, 0.3069, 0.3066, 0.3128, 0.3162, 0.3225,\n",
      "         0.3085, 0.3085, 0.3057, 0.2963, 0.3090, 0.3132, 0.3090, 0.3150, 0.3129,\n",
      "         0.3165]])\n",
      "tensor([[[0.3667, 0.3637, 0.3451,  ..., 0.3814, 0.4510, 0.4294],\n",
      "         [0.3990, 0.3706, 0.4118,  ..., 0.4235, 0.3314, 0.3176],\n",
      "         [0.3961, 0.3451, 0.3686,  ..., 0.4137, 0.4147, 0.3657],\n",
      "         ...,\n",
      "         [0.5490, 0.3608, 0.2922,  ..., 0.3255, 0.2971, 0.3147],\n",
      "         [0.4745, 0.3265, 0.3196,  ..., 0.3667, 0.3588, 0.3627],\n",
      "         [0.3657, 0.2882, 0.3206,  ..., 0.2833, 0.3363, 0.4000]],\n",
      "\n",
      "        [[0.3745, 0.3716, 0.3529,  ..., 0.3735, 0.4431, 0.4216],\n",
      "         [0.4069, 0.3784, 0.4196,  ..., 0.4157, 0.3235, 0.3098],\n",
      "         [0.4049, 0.3657, 0.3892,  ..., 0.4059, 0.4069, 0.3578],\n",
      "         ...,\n",
      "         [0.5275, 0.3314, 0.3029,  ..., 0.3451, 0.3167, 0.3343],\n",
      "         [0.4824, 0.3304, 0.3255,  ..., 0.4059, 0.3804, 0.3686],\n",
      "         [0.3588, 0.2931, 0.3275,  ..., 0.3225, 0.3578, 0.4049]],\n",
      "\n",
      "        [[0.3284, 0.3157, 0.3020,  ..., 0.3147, 0.3843, 0.3647],\n",
      "         [0.3618, 0.3225, 0.3676,  ..., 0.3647, 0.2735, 0.2588],\n",
      "         [0.3618, 0.3157, 0.3441,  ..., 0.3549, 0.3559, 0.3069],\n",
      "         ...,\n",
      "         [0.3931, 0.2500, 0.2480,  ..., 0.3176, 0.2892, 0.3078],\n",
      "         [0.3608, 0.3118, 0.2902,  ..., 0.3696, 0.3520, 0.3451],\n",
      "         [0.2559, 0.2627, 0.2853,  ..., 0.2873, 0.3304, 0.3804]]])\n"
     ]
    }
   ],
   "source": [
    "torch.mean(dataset[0][0],dim=1)\n",
    "print(\n",
    "torch.mean(dataset[0][0],dim=1)\n",
    ")\n",
    "print(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6a324a96-bb33-4b01-a546-37bee25c8e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.tensor([[[1.,2.,3.],[4.,5.,6.],[7.,8.,9.]],[[11.,12.,13.],[14.,15.,16.],[17.,18.,19.]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d2a5db53-f172-422c-a2c5-f29799a6e6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.],\n",
      "         [ 4.,  5.,  6.],\n",
      "         [ 7.,  8.,  9.]],\n",
      "\n",
      "        [[11., 12., 13.],\n",
      "         [14., 15., 16.],\n",
      "         [17., 18., 19.]]])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2bd2e408-edf7-4532-bab2-09a2520e3c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  7.,  8.],\n",
       "        [ 9., 10., 11.],\n",
       "        [12., 13., 14.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(temp, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eeab13-497f-4bba-aa28-673ae4540f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
