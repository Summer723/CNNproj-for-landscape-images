{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadb02df-4542-4286-913c-d0dc6c4c1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms.functional as fn\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.nn.functional import one_hot as one_hot_encoding\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657cc984-76bb-4ebd-b674-620648de1833",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch = 256\n",
    "category = {\n",
    "            'forest': 0, 'buildings': 1, 'river': 2, 'mobilehomepark': 3,\n",
    "             'harbor': 4, 'golfcourse': 5, 'agricultural': 6, \n",
    "             'runway': 7, 'baseballdiamond' : 8, 'overpass' : 9, 'chaparral':10, \n",
    "             'tenniscourt':11, 'intersection':12, 'airplane':13, 'parkinglot':14, \n",
    "             'sparseresidential':15, 'mediumresidential':16, 'denseresidential':17, \n",
    "             'beach':18, 'freeway':19, 'storagetanks':20\n",
    "}\n",
    "path = \"/Users/summer/desktop/CNNproj-for-landscape-images/data/UCMerced_LandUse/WholeDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554170b-22c2-46c9-a8d1-5f85d8b99049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset I created \n",
    "# easier to use dataloader \n",
    "\n",
    "class LandscapeDataset(Dataset):\n",
    "    # here label is a 100 element tensor\n",
    "    def __init__(self, label, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = label\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_data = self.img_dir + \"/\" + os.listdir(self.img_dir)[idx]\n",
    "        image = Image.open(path_to_data)\n",
    "        \n",
    "        # make it a tensor\n",
    "        image = ToTensor()(image)\n",
    "        label = self.img_labels[idx]\n",
    "        \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629a2aa-71e2-4250-a203-e66eeb6489c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {\n",
    "            'forest': 0, 'buildings': 1, 'river': 2, 'mobilehomepark': 3,\n",
    "             'harbor': 4, 'golfcourse': 5, 'agricultural': 6, \n",
    "             'runway': 7, 'baseballdiamond' : 8, 'overpass' : 9, 'chaparral':10, \n",
    "             'tenniscourt':11, 'intersection':12, 'airplane':13, 'parkinglot':14, \n",
    "             'sparseresidential':15, 'mediumresidential':16, 'denseresidential':17, \n",
    "             'beach':18, 'freeway':19, 'storagetanks':20\n",
    "}\n",
    "path = \"/Users/summer/desktop/CNNproj-for-landscape-images/data/UCMerced_LandUse/WholeDataset\"\n",
    "\n",
    "data = os.listdir(path)\n",
    "labels = torch.zeros(len(data))\n",
    "# try to find the class that image belongs to\n",
    "for i in range (len(data)):\n",
    "    labels[i] = category[re.findall('([a-zA-Z ]+)\\d*.*', data[i])[0]]\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize([64,64]),\n",
    "    # normalize it, get from mean and std from \n",
    "    #torchvision.transforms.Normalize([0.4858 , 0.4909, 0.4521],[0.2117, 0.1953, 0.1896]),\n",
    "    #torchvision.transforms.Normalize([0.9, 0.9, 0.9],[0.2117, 0.1953, 0.1896]),\n",
    "    \n",
    "]\n",
    ")\n",
    "labels = labels.long()\n",
    "dataset = LandscapeDataset(\n",
    "                            labels, \n",
    "                           \"/Users/summer/desktop/CNNproj-for-landscape-images/data/UCMerced_LandUse/WholeDataset\",\n",
    "                            transform = transform\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f2bf6-4c9e-41c2-9077-ac9cc4da2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set, test_set = torch.utils.data.random_split(dataset, \n",
    "                                                                       [1260, 420, 420], \n",
    "                                                                       generator=torch.Generator().manual_seed(42)\n",
    "                                                                      )\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=32, shuffle=True,drop_last =True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=32, shuffle=False,drop_last =True)\n",
    "\n",
    "plt.imshow(dataset[230][0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb269f1-30bb-4f3d-8d3d-f838a1d87221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, (3, 3),padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3,3),padding='same')\n",
    "        self.conv3 = nn.Conv2d(64,128,(3,3), padding=\"same\")\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.fc = nn.Linear(8192, 21)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.flatten(x) # flatten all dimensions except batch\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d83ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    \n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940fb8f7-2e8c-4bbb-9305-c2df786e9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "model = Net()\n",
    "EPOCHS = 100\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "best_vloss = 1_000_000.\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch_number)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(validation_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = model(vinputs)\n",
    "        accuracy = torch.sum(1 * (voutputs.argmax(dim=1) == vlabels)) /(voutputs.shape[1])\n",
    "        print(accuracy)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be971415-1afd-4f98-8f71-d2d3f195f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, (3, 3),padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3,3),padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d((2,2), stride=(2,2))\n",
    "        self.fc = nn.Linear(16384, 21)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.flatten(x) # flatten all dimensions except batch\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bb278b-e25e-4105-8d06-7673b422c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of target with class indices\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "\n",
    "output = loss(input, target)\n",
    "output.backward()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
